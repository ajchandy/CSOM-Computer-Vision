{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we shall train a model for classifying the digits using our own network, and then we shall how good/bad it is compared to the the one trained using transfer learning.\n",
    "\n",
    "We use the famous Digits Recoginition Challenge Dataset - MNIST readily avaliable from one of the frameworks. \n",
    "\n",
    "We shall follow the following steps: \n",
    "1. Load the Data (already split into train and test)\n",
    "2. Define a Convolutional Neural Network to train a Classification Problem for classifying fashion clothing\n",
    "3. Train a model using the training set\n",
    "4. Evluate the model on test set\n",
    "5. Repeat a similar approach for Tranfer Learning using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "\n",
    "# Mention the Class Name List\n",
    "class_list = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Convert a one-hot vector for the test-labels\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(class_list))\n",
    "\n",
    "\n",
    "# Split the test set to Validation & Test set\n",
    "num_test_samples = x_test.shape[0]\n",
    "x_val, y_val = x_test[0:num_test_samples//2,:,:], y_test[0:num_test_samples//2]\n",
    "x_test, y_test = x_test[num_test_samples//2:,:,:], y_test[num_test_samples//2:]\n",
    "\n",
    "x_val = x_val.reshape(5000,28,28,1)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=len(class_list))\n",
    "\n",
    "x_test = x_test.reshape(5000,28,28,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, We can see that we have 60000 images for training with 10 different class labels:<br>\n",
    "0   T-shirt/top <br>\n",
    "1   Trouser<br>\n",
    "2 \tPullover<br>\n",
    "3 \tDress<br>\n",
    "4 \tCoat<br>\n",
    "5 \tSandal<br>\n",
    "6 \tShirt<br>\n",
    "7 \tSneaker<br>\n",
    "8 \tBag<br>\n",
    "9 \tAnkle boot<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000, 10))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Data \n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 28, 28, 1), (5000, 10))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Data\n",
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1367afad0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = random.randint(0,60000)\n",
    "class_fig = class_list[np.argmax(y_train[num])]\n",
    "\n",
    "plt.imshow(x_train[num].reshape(28,28))\n",
    "print(\"Class Name :{}\".format(class_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Conv2D, Activation, Dense, Flatten, MaxPooling2D\n",
    "\n",
    "# Writing the model\n",
    "model = Sequential()\n",
    "\n",
    "inputs = Input(shape=(28, 28))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=3, strides=2,padding=\"same\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3), strides=(2,2),padding=\"same\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3), strides=(2,2),padding=\"same\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='softmax',use_bias=True))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 1.4006 - accuracy: 0.7575 - val_loss: 0.4771 - val_accuracy: 0.8274\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.3893 - accuracy: 0.8603 - val_loss: 0.4155 - val_accuracy: 0.8514\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.3411 - accuracy: 0.8769 - val_loss: 0.3944 - val_accuracy: 0.8638\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.3104 - accuracy: 0.8862 - val_loss: 0.3808 - val_accuracy: 0.8702\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 0.2878 - accuracy: 0.8947 - val_loss: 0.3958 - val_accuracy: 0.8632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x12c244350>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val,y_val), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.24"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on Test Set\n",
    "y_pred = model.predict_classes(x_test)\n",
    "acc = np.sum(y_test == y_pred) / y_test.shape[0]\n",
    "#Print Accuracy\n",
    "print(\"Test Accuracy :{}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 93,962\n",
      "Trainable params: 93,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
